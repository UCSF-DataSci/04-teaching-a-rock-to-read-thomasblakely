{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "466204de",
   "metadata": {},
   "source": [
    "# The Lazy Book Report\n",
    "\n",
    "Your professor has assigned a book report on \"The Red-Headed League\" by Arthur Conan Doyle. \n",
    "\n",
    "You haven't read the book. And out of stubbornness, you won't.\n",
    "\n",
    "But you *have* learned NLP. Let's use it to answer the professor's questions without reading.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, let's fetch the text from Project Gutenberg and prepare it for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "08a46884",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Story loaded: 4000 words in 3 sections\n",
      "Section sizes: [1333, 1333, 1334]\n"
     ]
    }
   ],
   "source": [
    "# Fetch and prepare text - RUN THIS CELL FIRST\n",
    "import os\n",
    "import urllib.request\n",
    "import re\n",
    "\n",
    "os.makedirs(\"output\", exist_ok=True)\n",
    "\n",
    "url = 'https://www.gutenberg.org/files/1661/1661-0.txt'\n",
    "req = urllib.request.Request(url, headers={'User-Agent': 'Python-urllib'})\n",
    "with urllib.request.urlopen(req, timeout=30) as resp:\n",
    "    text = resp.read().decode('utf-8')\n",
    "\n",
    "# Strip Gutenberg boilerplate\n",
    "text = text.split('*** START OF')[1].split('***')[1]\n",
    "text = text.split('*** END OF')[0]\n",
    "\n",
    "# Extract \"The Red-Headed League\" story (it's the second story in the collection)\n",
    "matches = list(re.finditer(r'THE RED-HEADED LEAGUE', text, re.IGNORECASE))\n",
    "story_start = matches[1].end()\n",
    "story_text = text[story_start:]\n",
    "story_end = re.search(r'\\n\\s*III\\.\\s*\\n', story_text)\n",
    "story_text = story_text[:story_end.start()] if story_end else story_text\n",
    "\n",
    "# Split into 3 sections by word count\n",
    "words = story_text.split()[:4000]\n",
    "section_size = len(words) // 3\n",
    "sections = [\n",
    "    ' '.join(words[:section_size]),\n",
    "    ' '.join(words[section_size:2*section_size]),\n",
    "    ' '.join(words[2*section_size:])\n",
    "]\n",
    "\n",
    "print(f\"Story loaded: {len(words)} words in {len(sections)} sections\")\n",
    "print(f\"Section sizes: {[len(s.split()) for s in sections]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9e757a",
   "metadata": {},
   "source": [
    "## Professor's Questions\n",
    "\n",
    "Your professor wants you to answer 5 questions about the story. Let's use NLP to find the answers.\n",
    "\n",
    "---\n",
    "\n",
    "## Question 1: Writing Style\n",
    "\n",
    "> \"This text is from the 1890s. What makes it different from modern writing?\"\n",
    "\n",
    "**NLP Method:** Use preprocessing to compute text statistics. Tokenize the text and calculate:\n",
    "- Vocabulary richness (unique words / total words)\n",
    "- Average sentence length\n",
    "- Average word length\n",
    "\n",
    "**Hint:** Formal, literary writing typically shows higher vocabulary richness and longer sentences than modern casual text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1710e297",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary Richness: 0.0997\n",
      "Average Sentence Length: 15.53 words\n",
      "Average Word Length: 4.19 characters\n"
     ]
    }
   ],
   "source": [
    "# import string, import re\n",
    "import string\n",
    "import re\n",
    "\n",
    "# Tokenize\n",
    "words_tokenized = story_text.translate(str.maketrans('', '', string.punctuation)).lower().split()\n",
    "\n",
    "# Sentences\n",
    "## find_nuisance_words = set(re.findall(r'\\b([A-Z][a-z]{0,3}\\.)\\s+\\w', story_text)) | Initial check for abbreviations\n",
    "## Since formal, literary writing = lots of titles = can't split on periods\n",
    "find_nuisance_words = set(re.findall(r'\\b(Mr\\.|Mrs\\.|Dr\\.|St\\.|etc\\.)', story_text))\n",
    "text_flag = story_text\n",
    "for nw in find_nuisance_words:\n",
    "    text_flag = text_flag.replace(nw, nw.replace('.', 'PROT'))\n",
    "sentences = re.findall(r'[^.!?]+[.!?]', text_flag)\n",
    "sentences = [s.replace('PROT', '.').strip() for s in sentences if s.strip()]\n",
    "\n",
    "# Calculate vocab_richness, avg_sentence_length, avg_word_length\n",
    "vocab_richness = len(set(words_tokenized)) / len(words_tokenized)\n",
    "avg_sentence_length = len(words_tokenized) / len(sentences)\n",
    "avg_word_length = sum(len(word) for word in words_tokenized) / len(words_tokenized)\n",
    "\n",
    "# Output Results\n",
    "print(f\"Vocabulary Richness: {vocab_richness:.4f}\")\n",
    "print(f\"Average Sentence Length: {avg_sentence_length:.2f} words\")\n",
    "print(f\"Average Word Length: {avg_word_length:.2f} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5545405",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 2: Main Characters\n",
    "\n",
    "> \"Who are the main characters in this story?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract PERSON entities.\n",
    "\n",
    "**Hint:** Use spaCy's `en_core_web_sm` model. Process the text and filter entities where `ent.label_ == 'PERSON'`. Count how often each name appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "332a59da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract PERSON entities using spaCy NER\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(story_text)\n",
    "people = [ent.text for ent in doc.ents if ent.label_ == \"PERSON\"]\n",
    "characters = {name: people.count(name) for name in set(people)}\n",
    "\n",
    "# Save findings: \n",
    "with open(\"output/characters.txt\", \"w\") as f:\n",
    "    for name, counts in characters.items():\n",
    "        f.write(f\"{name}: {counts}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6732e661",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 3: Story Locations\n",
    "\n",
    "> \"Where does the story take place?\"\n",
    "\n",
    "**NLP Method:** Use Named Entity Recognition (NER) to extract location entities (GPE and LOC).\n",
    "\n",
    "**Hint:** Filter entities where `ent.label_` is 'GPE' (geopolitical entity) or 'LOC' (location)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bc3f8f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Your code here: extract GPE and LOC entities using spaCy NER\n",
    "locations = set([ent.text for ent in doc.ents if ent.label_ in (\"GPE\", \"LOC\")])\n",
    "\n",
    "# When done, save your findings:\n",
    "with open(\"output/locations.txt\", \"w\") as f:\n",
    "    for place in locations:\n",
    "        f.write(f\"{place}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da4b228d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 4: Wilson's Business\n",
    "\n",
    "> \"What is Wilson's business?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's business.\n",
    "\n",
    "**Hint:** Create a TF-IDF vectorizer, fit it on the 3 sections, then transform your query using the same vectorizer (`.transform()`, not `.fit_transform()` - you want to use the vocabulary learned from the sections). Find which section has the highest cosine similarity and read it to find the answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "24f7fb77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03360193 0.06577618 0.04978973]\n",
      "Sherlock Holmes,” said Jabez Wilson, mopping his forehead; “I have a small pawnbroker’s business at Coburg Square, near the City.\n"
     ]
    }
   ],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# Import libraries\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# Standard setup for vectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(sections)\n",
    "\n",
    "# Using Wilson's Business as Query\n",
    "looking_for = \"Wilson's business\"\n",
    "looking_vec = vectorizer.transform([looking_for])\n",
    "cosine = cosine_similarity(looking_vec, X).flatten()\n",
    "print(cosine)\n",
    "\n",
    "# Get highest similarlity from cosine, choose section for analysis, split sentences\n",
    "highest_similarity = cosine.argmax()\n",
    "section = sections[highest_similarity]\n",
    "sentences_section = re.findall(r'[^.!?]+[.!?]', section)  # Match via punctuation only allow ending punct.\n",
    "\n",
    "# Iterate over each sentence, remove white space, check for \"business\", if present print the relevant sentence\n",
    "for sentence in sentences_section:\n",
    "    sentence = sentence.strip()\n",
    "    if \"business\" in sentence.lower():\n",
    "        print(sentence)\n",
    "        break\n",
    "\n",
    "# Save findings:\n",
    "with open(\"output/business.txt\", \"w\") as f:\n",
    "    f.write(\"Wilson's business is: a small pawnbroker’s business at Coburg Square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85452bf1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Question 5: Wilson's Work Routine\n",
    "\n",
    "> \"What is Wilson's daily work routine for the League?\"\n",
    "\n",
    "**NLP Method:** Use TF-IDF similarity to find which section discusses Wilson's work routine.\n",
    "\n",
    "**Hint:** Similar to Question 4 - use TF-IDF to find the section that best matches your query about work routine. The answer includes what Wilson had to do and what eventually happened."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0ddea14a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.03942181 0.0336671  0.04271273]\n",
      "Searching for: ['wilson', 'daily', 'work', 'routine', 'league']\n",
      "jabez wilson,’ said my assistant, ‘and he is willing to fill a vacancy in the league. ’ “‘And he is admirably suited for it,’ the other answered. \n",
      "\n",
      "wilson?  Have you a family? \n",
      "\n",
      "wilson! ’ said Vincent Spaulding. \n",
      "\n",
      "’ “‘and the work? ’ “‘Is purely nominal. \n",
      "\n",
      "’ “‘and the work? ’ “‘Is to copy out the _Encyclopædia Britannica_. \n",
      "\n",
      "jabez wilson, and let me congratulate you once more on the important position which you have been fortunate enough to gain. ’ He bowed me out of the room and I went home with my assistant, hardly knowing what to say or do, I was so pleased at my own good fortune. \n",
      "\n",
      "duncan ross was there to see that i got fairly to work.  He started me off upon the letter A, and then he left me; but he would drop in from time to time to see that all was right with me. \n",
      "\n",
      "holmes, and on saturday the manager came in and planked down four golden sovereigns for my week’s work.  It was the same next week, and the same the week after. \n",
      "\n",
      "i went to my work as usual at ten o’clock, but the door was shut and locked, with a little square of cardboard hammered on to the middle of the panel with a tack.  Here it is, and you can read for yourself. \n",
      "\n",
      "it read in this fashion: “the red-headed league is dissolved.  October 9, 1890. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code here: use TF-IDF similarity to find the relevant section\n",
    "\n",
    "# Use Wilson Daily Work Routine \n",
    "query = \"Wilson daily work routine League\"\n",
    "\n",
    "# Follow same steps as before\n",
    "query_vec = vectorizer.transform([query])\n",
    "cosine = cosine_similarity(query_vec, X).flatten()\n",
    "print(cosine)\n",
    "highest_similarity = cosine.argmax()\n",
    "section = sections[highest_similarity]\n",
    "sentences_in_section = re.findall(r'[^.!?]+[.!?]', section)\n",
    "\n",
    "# Get query words (lowercase, filter short words)\n",
    "query_words = [w.lower() for w in query.split()]\n",
    "print(f\"Searching for: {query_words}\")\n",
    "\n",
    "# Find sentences containing any query word. Print next sentence as well, \n",
    "# because 'and the work?' seems like it has follow-up\n",
    "for next, sentence in enumerate(sentences_in_section):\n",
    "    sentence = sentence.strip().lower()\n",
    "    if any(word in sentence for word in query_words):\n",
    "        print(sentence, sentences_in_section[next + 1], '\\n')\n",
    "\n",
    "# Save findings\n",
    "with open(\"output/routine.txt\", \"w\") as f:\n",
    "    f.write(\"Wilson's work routine: Is to copy out the _Encyclopædia Britannica_\\n\")\n",
    "    f.write(\"What happened: the red-headed league is dissolved\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
